{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print \"Epoch %s training complete\" % j\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print \"Cost on training data: {}\".format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print \"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print \"Cost on evaluation data: {}\".format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print \"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data)\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data.ix[:,0]\n",
    "X = data.ix[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.1, random_state=0, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = [(x.reshape(784,1), vectorized_result(y)) for x,y in zip(X_train.values, y_train.values)]\n",
    "test_data = [(x.reshape(784,1), y) for x,y in zip(X_test.values, y_test.values)]\n",
    "valid_data = [(x.reshape(784,1), y) for x,y in zip(X_valid.values, y_valid.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10], cost = CrossEntropyCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 1.39695336692\n",
      "Accuracy on training data: 28820 / 34019\n",
      "Cost on evaluation data: 1.41763261788\n",
      "Accuracy on evaluation data: 3170 / 3782\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.23067755794\n",
      "Accuracy on training data: 29410 / 34019\n",
      "Cost on evaluation data: 1.25394115674\n",
      "Accuracy on evaluation data: 3221 / 3782\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.12952954995\n",
      "Accuracy on training data: 29629 / 34019\n",
      "Cost on evaluation data: 1.14767698612\n",
      "Accuracy on evaluation data: 3271 / 3782\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.05034237555\n",
      "Accuracy on training data: 29624 / 34019\n",
      "Cost on evaluation data: 1.07618324411\n",
      "Accuracy on evaluation data: 3252 / 3782\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.961506957919\n",
      "Accuracy on training data: 29932 / 34019\n",
      "Cost on evaluation data: 0.977664134231\n",
      "Accuracy on evaluation data: 3313 / 3782\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.918859649571\n",
      "Accuracy on training data: 30029 / 34019\n",
      "Cost on evaluation data: 0.941484980992\n",
      "Accuracy on evaluation data: 3310 / 3782\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.91112400816\n",
      "Accuracy on training data: 30075 / 34019\n",
      "Cost on evaluation data: 0.936269224906\n",
      "Accuracy on evaluation data: 3316 / 3782\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.865469926684\n",
      "Accuracy on training data: 30173 / 34019\n",
      "Cost on evaluation data: 0.896098401562\n",
      "Accuracy on evaluation data: 3316 / 3782\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.86891347842\n",
      "Accuracy on training data: 30096 / 34019\n",
      "Cost on evaluation data: 0.915200710732\n",
      "Accuracy on evaluation data: 3293 / 3782\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.844442462186\n",
      "Accuracy on training data: 30002 / 34019\n",
      "Cost on evaluation data: 0.869217603868\n",
      "Accuracy on evaluation data: 3341 / 3782\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.771095144142\n",
      "Accuracy on training data: 30448 / 34019\n",
      "Cost on evaluation data: 0.81939851002\n",
      "Accuracy on evaluation data: 3342 / 3782\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.778625948433\n",
      "Accuracy on training data: 30430 / 34019\n",
      "Cost on evaluation data: 0.820624787342\n",
      "Accuracy on evaluation data: 3346 / 3782\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.779513187992\n",
      "Accuracy on training data: 30406 / 34019\n",
      "Cost on evaluation data: 0.809148063973\n",
      "Accuracy on evaluation data: 3354 / 3782\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.756048165886\n",
      "Accuracy on training data: 30380 / 34019\n",
      "Cost on evaluation data: 0.787585277422\n",
      "Accuracy on evaluation data: 3342 / 3782\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.730894322045\n",
      "Accuracy on training data: 30517 / 34019\n",
      "Cost on evaluation data: 0.771232391491\n",
      "Accuracy on evaluation data: 3350 / 3782\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.68170962397\n",
      "Accuracy on training data: 30738 / 34019\n",
      "Cost on evaluation data: 0.719335532518\n",
      "Accuracy on evaluation data: 3388 / 3782\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.676515857332\n",
      "Accuracy on training data: 30676 / 34019\n",
      "Cost on evaluation data: 0.718902639773\n",
      "Accuracy on evaluation data: 3364 / 3782\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.694355035646\n",
      "Accuracy on training data: 30643 / 34019\n",
      "Cost on evaluation data: 0.736396751244\n",
      "Accuracy on evaluation data: 3376 / 3782\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.724730115341\n",
      "Accuracy on training data: 30483 / 34019\n",
      "Cost on evaluation data: 0.768655347941\n",
      "Accuracy on evaluation data: 3333 / 3782\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.729671454525\n",
      "Accuracy on training data: 30430 / 34019\n",
      "Cost on evaluation data: 0.759619380345\n",
      "Accuracy on evaluation data: 3363 / 3782\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.689155674054\n",
      "Accuracy on training data: 30627 / 34019\n",
      "Cost on evaluation data: 0.735645893961\n",
      "Accuracy on evaluation data: 3375 / 3782\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.67432989442\n",
      "Accuracy on training data: 30732 / 34019\n",
      "Cost on evaluation data: 0.722049506018\n",
      "Accuracy on evaluation data: 3357 / 3782\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.629321398171\n",
      "Accuracy on training data: 30889 / 34019\n",
      "Cost on evaluation data: 0.67650274417\n",
      "Accuracy on evaluation data: 3392 / 3782\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.651338326743\n",
      "Accuracy on training data: 30742 / 34019\n",
      "Cost on evaluation data: 0.695761502847\n",
      "Accuracy on evaluation data: 3385 / 3782\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.645000147554\n",
      "Accuracy on training data: 30800 / 34019\n",
      "Cost on evaluation data: 0.702325911732\n",
      "Accuracy on evaluation data: 3374 / 3782\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.657804433471\n",
      "Accuracy on training data: 30674 / 34019\n",
      "Cost on evaluation data: 0.719158233781\n",
      "Accuracy on evaluation data: 3347 / 3782\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.650318395028\n",
      "Accuracy on training data: 30696 / 34019\n",
      "Cost on evaluation data: 0.706598869977\n",
      "Accuracy on evaluation data: 3383 / 3782\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.63309568205\n",
      "Accuracy on training data: 30933 / 34019\n",
      "Cost on evaluation data: 0.695810256555\n",
      "Accuracy on evaluation data: 3382 / 3782\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.616882584618\n",
      "Accuracy on training data: 30962 / 34019\n",
      "Cost on evaluation data: 0.66085793128\n",
      "Accuracy on evaluation data: 3412 / 3782\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.693677408673\n",
      "Accuracy on training data: 30528 / 34019\n",
      "Cost on evaluation data: 0.734009327389\n",
      "Accuracy on evaluation data: 3355 / 3782\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.631142766988\n",
      "Accuracy on training data: 30880 / 34019\n",
      "Cost on evaluation data: 0.675544233479\n",
      "Accuracy on evaluation data: 3407 / 3782\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.615103204146\n",
      "Accuracy on training data: 30920 / 34019\n",
      "Cost on evaluation data: 0.682467055306\n",
      "Accuracy on evaluation data: 3395 / 3782\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.64996901979\n",
      "Accuracy on training data: 30765 / 34019\n",
      "Cost on evaluation data: 0.696697819816\n",
      "Accuracy on evaluation data: 3385 / 3782\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.619930640976\n",
      "Accuracy on training data: 30947 / 34019\n",
      "Cost on evaluation data: 0.675269296638\n",
      "Accuracy on evaluation data: 3409 / 3782\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.617590891905\n",
      "Accuracy on training data: 30936 / 34019\n",
      "Cost on evaluation data: 0.672039180723\n",
      "Accuracy on evaluation data: 3403 / 3782\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.602149594514\n",
      "Accuracy on training data: 31011 / 34019\n",
      "Cost on evaluation data: 0.66235611322\n",
      "Accuracy on evaluation data: 3397 / 3782\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.650045938426\n",
      "Accuracy on training data: 30669 / 34019\n",
      "Cost on evaluation data: 0.697073602971\n",
      "Accuracy on evaluation data: 3375 / 3782\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.600324461005\n",
      "Accuracy on training data: 30925 / 34019\n",
      "Cost on evaluation data: 0.651452494359\n",
      "Accuracy on evaluation data: 3386 / 3782\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.5828900021\n",
      "Accuracy on training data: 30966 / 34019\n",
      "Cost on evaluation data: 0.641118984573\n",
      "Accuracy on evaluation data: 3399 / 3782\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.57275904609\n",
      "Accuracy on training data: 31111 / 34019\n",
      "Cost on evaluation data: 0.621847235544\n",
      "Accuracy on evaluation data: 3410 / 3782\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.619880343205\n",
      "Accuracy on training data: 30921 / 34019\n",
      "Cost on evaluation data: 0.646771263982\n",
      "Accuracy on evaluation data: 3407 / 3782\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.605166257764\n",
      "Accuracy on training data: 30924 / 34019\n",
      "Cost on evaluation data: 0.646301659632\n",
      "Accuracy on evaluation data: 3399 / 3782\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.573920567145\n",
      "Accuracy on training data: 31079 / 34019\n",
      "Cost on evaluation data: 0.616479134402\n",
      "Accuracy on evaluation data: 3440 / 3782\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.633388192872\n",
      "Accuracy on training data: 30793 / 34019\n",
      "Cost on evaluation data: 0.673726616722\n",
      "Accuracy on evaluation data: 3397 / 3782\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.568226744133\n",
      "Accuracy on training data: 31160 / 34019\n",
      "Cost on evaluation data: 0.618552637928\n",
      "Accuracy on evaluation data: 3416 / 3782\n",
      "\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.604837456599\n",
      "Accuracy on training data: 30905 / 34019\n",
      "Cost on evaluation data: 0.662065659494\n",
      "Accuracy on evaluation data: 3401 / 3782\n",
      "\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.60374796473\n",
      "Accuracy on training data: 31091 / 34019\n",
      "Cost on evaluation data: 0.655290240961\n",
      "Accuracy on evaluation data: 3420 / 3782\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.635706009876\n",
      "Accuracy on training data: 30763 / 34019\n",
      "Cost on evaluation data: 0.702198057259\n",
      "Accuracy on evaluation data: 3368 / 3782\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.573469298437\n",
      "Accuracy on training data: 31098 / 34019\n",
      "Cost on evaluation data: 0.614814248431\n",
      "Accuracy on evaluation data: 3427 / 3782\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.554867007823\n",
      "Accuracy on training data: 31225 / 34019\n",
      "Cost on evaluation data: 0.614173880345\n",
      "Accuracy on evaluation data: 3429 / 3782\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.557856463916\n",
      "Accuracy on training data: 31198 / 34019\n",
      "Cost on evaluation data: 0.604002417003\n",
      "Accuracy on evaluation data: 3440 / 3782\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.57040097358\n",
      "Accuracy on training data: 31172 / 34019\n",
      "Cost on evaluation data: 0.621792600005\n",
      "Accuracy on evaluation data: 3409 / 3782\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.571129241955\n",
      "Accuracy on training data: 31154 / 34019\n",
      "Cost on evaluation data: 0.61594423404\n",
      "Accuracy on evaluation data: 3436 / 3782\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.571406076854\n",
      "Accuracy on training data: 31119 / 34019\n",
      "Cost on evaluation data: 0.619794636475\n",
      "Accuracy on evaluation data: 3426 / 3782\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.528867962204\n",
      "Accuracy on training data: 31299 / 34019\n",
      "Cost on evaluation data: 0.582388603771\n",
      "Accuracy on evaluation data: 3425 / 3782\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.539875886372\n",
      "Accuracy on training data: 31221 / 34019\n",
      "Cost on evaluation data: 0.592475260948\n",
      "Accuracy on evaluation data: 3437 / 3782\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.565218596492\n",
      "Accuracy on training data: 31121 / 34019\n",
      "Cost on evaluation data: 0.621031648555\n",
      "Accuracy on evaluation data: 3427 / 3782\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.576425502338\n",
      "Accuracy on training data: 31145 / 34019\n",
      "Cost on evaluation data: 0.627836867238\n",
      "Accuracy on evaluation data: 3415 / 3782\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.565805945792\n",
      "Accuracy on training data: 31184 / 34019\n",
      "Cost on evaluation data: 0.612487722414\n",
      "Accuracy on evaluation data: 3430 / 3782\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.533919608388\n",
      "Accuracy on training data: 31329 / 34019\n",
      "Cost on evaluation data: 0.598887418555\n",
      "Accuracy on evaluation data: 3427 / 3782\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.602037556828\n",
      "Accuracy on training data: 30944 / 34019\n",
      "Cost on evaluation data: 0.653091365442\n",
      "Accuracy on evaluation data: 3405 / 3782\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.551737440606\n",
      "Accuracy on training data: 31220 / 34019\n",
      "Cost on evaluation data: 0.606251259706\n",
      "Accuracy on evaluation data: 3427 / 3782\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.544209843676\n",
      "Accuracy on training data: 31233 / 34019\n",
      "Cost on evaluation data: 0.592035390263\n",
      "Accuracy on evaluation data: 3429 / 3782\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.540997808458\n",
      "Accuracy on training data: 31280 / 34019\n",
      "Cost on evaluation data: 0.589252713943\n",
      "Accuracy on evaluation data: 3450 / 3782\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.514408306975\n",
      "Accuracy on training data: 31367 / 34019\n",
      "Cost on evaluation data: 0.563745282225\n",
      "Accuracy on evaluation data: 3467 / 3782\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.520791990038\n",
      "Accuracy on training data: 31328 / 34019\n",
      "Cost on evaluation data: 0.574494316755\n",
      "Accuracy on evaluation data: 3458 / 3782\n",
      "\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.552260447702\n",
      "Accuracy on training data: 31140 / 34019\n",
      "Cost on evaluation data: 0.605372960128\n",
      "Accuracy on evaluation data: 3431 / 3782\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.533497158221\n",
      "Accuracy on training data: 31285 / 34019\n",
      "Cost on evaluation data: 0.590515566009\n",
      "Accuracy on evaluation data: 3462 / 3782\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.509290760646\n",
      "Accuracy on training data: 31301 / 34019\n",
      "Cost on evaluation data: 0.575825910551\n",
      "Accuracy on evaluation data: 3437 / 3782\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.571192125504\n",
      "Accuracy on training data: 31171 / 34019\n",
      "Cost on evaluation data: 0.615754717437\n",
      "Accuracy on evaluation data: 3433 / 3782\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.512635936657\n",
      "Accuracy on training data: 31447 / 34019\n",
      "Cost on evaluation data: 0.576864777633\n",
      "Accuracy on evaluation data: 3449 / 3782\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.507237672857\n",
      "Accuracy on training data: 31361 / 34019\n",
      "Cost on evaluation data: 0.570206529162\n",
      "Accuracy on evaluation data: 3437 / 3782\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.528606580952\n",
      "Accuracy on training data: 31262 / 34019\n",
      "Cost on evaluation data: 0.601403156017\n",
      "Accuracy on evaluation data: 3436 / 3782\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.532408645741\n",
      "Accuracy on training data: 31312 / 34019\n",
      "Cost on evaluation data: 0.609986740579\n",
      "Accuracy on evaluation data: 3421 / 3782\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.515943218046\n",
      "Accuracy on training data: 31349 / 34019\n",
      "Cost on evaluation data: 0.578495260274\n",
      "Accuracy on evaluation data: 3457 / 3782\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.515403223493\n",
      "Accuracy on training data: 31424 / 34019\n",
      "Cost on evaluation data: 0.579657999127\n",
      "Accuracy on evaluation data: 3450 / 3782\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.524161897324\n",
      "Accuracy on training data: 31217 / 34019\n",
      "Cost on evaluation data: 0.582072739951\n",
      "Accuracy on evaluation data: 3436 / 3782\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.543324603255\n",
      "Accuracy on training data: 31293 / 34019\n",
      "Cost on evaluation data: 0.619692058631\n",
      "Accuracy on evaluation data: 3418 / 3782\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.520242756535\n",
      "Accuracy on training data: 31360 / 34019\n",
      "Cost on evaluation data: 0.582052322883\n",
      "Accuracy on evaluation data: 3460 / 3782\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.494083604586\n",
      "Accuracy on training data: 31497 / 34019\n",
      "Cost on evaluation data: 0.570087531814\n",
      "Accuracy on evaluation data: 3456 / 3782\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1661ba58>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8719be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs=80\n",
    "evaluation_cost, evaluation_accuracy,training_cost, training_accuracy = net.SGD(training_data, num_epochs, 5, 0.001, lmbda=0.001, evaluation_data = valid_data, \n",
    "       monitor_evaluation_accuracy=True, monitor_evaluation_cost=True,\n",
    "    monitor_training_accuracy=True, monitor_training_cost=True)\n",
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "plt.plot(xrange(len(training_cost)), training_cost)\n",
    "plt.subplot(122)\n",
    "plt.plot(xrange(num_epochs), evaluation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9128363896165754"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.accuracy(test_data)/float(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
